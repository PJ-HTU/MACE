"""
SI Operator (Specialization Improvement Operator)
å¯¹åº”MACEè®ºæ–‡çš„SIç®—å­ - Algorithm 2, line 5

æ ¸å¿ƒé€»è¾‘:
- é€‰æ‹©å˜å¼‚ç³»æ•°(CV)æœ€å°çš„å®ä¾‹
- é’ˆå¯¹è¯¥å®ä¾‹ç”Ÿæˆä¸“ä¸šåŒ–çš„å¯å‘å¼
"""

import os
import numpy as np
from typing import Dict, List, Tuple
from langchain_core.prompts import PromptTemplate
from openai import OpenAI


class SIOperator:
    """
    SI (Specialization Improvement) ç®—å­
    
    è®ºæ–‡å…¬å¼(5): i* = arg min CV_i
    å…¶ä¸­ CV_i = std({f_i(h) : h âˆˆ H}) / mean({f_i(h) : h âˆˆ H})
    
    é€‰æ‹©å˜å¼‚ç³»æ•°æœ€å°çš„å®ä¾‹(ç®—æ³•è¦†ç›–ä¸è¶³),ç”Ÿæˆä¸“ä¸šåŒ–å¯å‘å¼
    """
    
    def __init__(
        self,
        problem: str,
        heuristic_dir: str,
        task_description_file: str,
        output_dir: str,
        test_data_dir: str,
        api_key: str,
        model: str
    ):
        """
        Args:
            problem: é—®é¢˜ç±»å‹ (tsp, jssp, cvrp, psp)
            heuristic_dir: å¯å‘å¼ä»£ç ç›®å½•
            task_description_file: ä»»åŠ¡æè¿°æ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            test_data_dir: æµ‹è¯•æ•°æ®ç›®å½• (ç”¨äºè¯»å–å®ä¾‹æ–‡ä»¶)
            api_key: LLM APIå¯†é’¥
            model: LLMæ¨¡å‹åç§°
        """
        self.problem = problem
        self.heuristic_dir = heuristic_dir
        self.task_description_file = task_description_file
        self.output_dir = output_dir
        self.test_data_dir = test_data_dir
        self.api_key = api_key
        self.model = model
        
        # åŠ è½½ä»»åŠ¡æè¿°
        self.task_description = self._load_task_description()
    
    def _load_task_description(self) -> str:
        """åŠ è½½ä»»åŠ¡æè¿°æ–‡ä»¶"""
        if self.task_description_file and os.path.exists(self.task_description_file):
            with open(self.task_description_file, 'r', encoding='utf-8') as f:
                return f.read()
        return ""
    
    def generate(self, population: List[Dict]) -> Tuple[str, str]:
        """
        æ‰§è¡ŒSIç®—å­å®Œæ•´å·¥ä½œæµ
        
        Args:
            population: å½“å‰ç§ç¾¤
                æ ¼å¼: [{'name': ..., 'performance_vector': [...], 'avg_performance': ...}, ...]
        
        Returns:
            (file_path, code): ç”Ÿæˆçš„å¯å‘å¼æ–‡ä»¶è·¯å¾„å’Œä»£ç 
        """
        print("\n" + "=" * 80)
        print("ğŸ¯ [SIç®—å­] Specialization Improvement å¼€å§‹")
        print("=" * 80)
        
        # Step 1: é€‰æ‹©CVæœ€å°çš„å®ä¾‹ (è®ºæ–‡å…¬å¼5)
        instance_idx, cv_value, instance_info = self._select_underserved_instance(population)
        print(f"âœ“ é€‰æ‹©çš„ç›®æ ‡å®ä¾‹: å®ä¾‹ #{instance_idx}")
        print(f"  å˜å¼‚ç³»æ•° CV: {cv_value:.4f} (æœ€å° - è¦†ç›–ä¸è¶³)")
        print(f"  å®ä¾‹ä¿¡æ¯: {instance_info}")
        
        # Step 2: åˆ†æè¯¥å®ä¾‹ä¸Šå„å¯å‘å¼çš„è¡¨ç°
        instance_analysis = self._analyze_instance_performance(population, instance_idx)
        
        # Step 3: ç”ŸæˆSIæç¤ºè¯
        si_prompt = self._create_si_prompt(instance_idx, instance_info, instance_analysis)
        
        # Step 4: è°ƒç”¨LLMç”Ÿæˆä¸“ä¸šåŒ–å¯å‘å¼
        llm_response = self._call_llm(si_prompt)
        
        # Step 5: æå–ä»£ç 
        extracted_code = self._extract_code_from_response(llm_response)
        
        if not extracted_code:
            print("âœ— æœªèƒ½ä»å“åº”ä¸­æå–ä»£ç ")
            print("\nå®Œæ•´å“åº”:")
            print(llm_response)
            return None, None
        
        # Step 6: ä¿å­˜ä»£ç 
        try:
            file_path = self._save_generated_heuristic(extracted_code)
            
            # ä¿å­˜å®Œæ•´å“åº”ï¼ˆåŒ…æ‹¬æ€è€ƒè¿‡ç¨‹ï¼‰
            response_file = file_path.replace('.py', '_full_response.txt')
            with open(response_file, 'w', encoding='utf-8') as f:
                f.write(f"Prompt used:\n{'-'*80}\n{si_prompt}\n\n")
                f.write(f"LLM Response:\n{'-'*80}\n{llm_response}")
            
            print(f"âœ“ ä»£ç å·²ä¿å­˜åˆ°: {file_path}")
            
        except Exception as e:
            print(f"âœ— ä¿å­˜å¤±è´¥: {str(e)}")
            return None, extracted_code
        
        # å®Œæˆ
        print("\n" + "=" * 80)
        print("âœ… [SIç®—å­] å·¥ä½œæµå®Œæˆ!")
        print("=" * 80)
        print(f"ğŸ“ ç”Ÿæˆçš„å¯å‘å¼ä»£ç : {file_path}")
        print(f"ğŸ“„ å®Œæ•´å“åº”è®°å½•: {response_file}")
        print(f"ğŸ”¬ ä¸“ä¸šåŒ–ç›®æ ‡: å®ä¾‹ #{instance_idx} (CV={cv_value:.4f})")
        print("=" * 80 + "\n")
        
        return file_path, extracted_code
    
    def _select_underserved_instance(self, population: List[Dict]) -> Tuple[int, float, str]:
        """
        é€‰æ‹©å˜å¼‚ç³»æ•°æœ€å°çš„å®ä¾‹ - è®ºæ–‡å…¬å¼(5)
        
        CV_i = std({f_i(h) : h âˆˆ H}) / mean({f_i(h) : h âˆˆ H})
        
        Args:
            population: å½“å‰ç§ç¾¤
        
        Returns:
            (instance_idx, cv_value, instance_info): å®ä¾‹ç´¢å¼•ã€CVå€¼ã€å®ä¾‹ä¿¡æ¯
        """
        if not population:
            raise ValueError("ç§ç¾¤ä¸ºç©º,æ— æ³•é€‰æ‹©å®ä¾‹")
        
        m = len(population[0]['performance_vector'])  # å®ä¾‹æ•°é‡
        
        # è®¡ç®—æ¯ä¸ªå®ä¾‹çš„å˜å¼‚ç³»æ•°
        cv_scores = {}
        
        for instance_idx in range(m):
            # è·å–æ‰€æœ‰å¯å‘å¼åœ¨è¯¥å®ä¾‹ä¸Šçš„æ€§èƒ½
            performances = [h['performance_vector'][instance_idx] for h in population]
            
            mean_perf = np.mean(performances)
            std_perf = np.std(performances)
            
            # è®¡ç®—å˜å¼‚ç³»æ•° CV = std / mean
            if mean_perf > 0:
                cv = std_perf / mean_perf
            else:
                cv = float('inf')  # é¿å…é™¤é›¶
            
            cv_scores[instance_idx] = cv
        
        # é€‰æ‹©CVæœ€å°çš„å®ä¾‹ (ç®—æ³•è¦†ç›–ä¸è¶³,æ€§èƒ½å·®å¼‚å°)
        target_instance_idx = min(cv_scores, key=cv_scores.get)
        target_cv = cv_scores[target_instance_idx]
        
        # è·å–å®ä¾‹ä¿¡æ¯
        instance_info = self._get_instance_info(target_instance_idx)
        
        return target_instance_idx, target_cv, instance_info
    
    def _get_instance_info(self, instance_idx: int) -> str:
        """
        è·å–å®ä¾‹çš„æè¿°ä¿¡æ¯
        
        Args:
            instance_idx: å®ä¾‹ç´¢å¼•
        
        Returns:
            instance_info: å®ä¾‹æè¿°
        """
        # å°è¯•è¯»å–å®ä¾‹æ–‡ä»¶å
        try:
            # å‡è®¾test_dataç›®å½•ä¸‹çš„æ–‡ä»¶æŒ‰é¡ºåºå¯¹åº”å®ä¾‹ç´¢å¼•
            if os.path.exists(self.test_data_dir):
                files = sorted([f for f in os.listdir(self.test_data_dir) 
                              if not f.startswith('.')])
                if instance_idx < len(files):
                    instance_file = files[instance_idx]
                    return f"æ–‡ä»¶: {instance_file}"
        except Exception as e:
            print(f"âš ï¸ æ— æ³•è¯»å–å®ä¾‹ä¿¡æ¯: {e}")
        
        return f"å®ä¾‹ç´¢å¼•: {instance_idx}"
    
    def _analyze_instance_performance(self, population: List[Dict], instance_idx: int) -> str:
        """
        åˆ†æå„å¯å‘å¼åœ¨ç›®æ ‡å®ä¾‹ä¸Šçš„è¡¨ç°
        
        Args:
            population: å½“å‰ç§ç¾¤
            instance_idx: ç›®æ ‡å®ä¾‹ç´¢å¼•
        
        Returns:
            analysis: æ€§èƒ½åˆ†ææ–‡æœ¬
        """
        performances = []
        
        for h in population:
            perf = h['performance_vector'][instance_idx]
            performances.append({
                'name': os.path.basename(h['name']),
                'performance': perf
            })
        
        # æŒ‰æ€§èƒ½æ’åº
        performances.sort(key=lambda x: x['performance'])
        
        # ç”Ÿæˆåˆ†ææ–‡æœ¬
        analysis = f"ç›®æ ‡å®ä¾‹ #{instance_idx} ä¸Šçš„æ€§èƒ½åˆ†æ:\n"
        analysis += f"  æœ€ä½³æ€§èƒ½: {performances[0]['performance']:.2f} ({performances[0]['name']})\n"
        analysis += f"  æœ€å·®æ€§èƒ½: {performances[-1]['performance']:.2f} ({performances[-1]['name']})\n"
        analysis += f"  å¹³å‡æ€§èƒ½: {np.mean([p['performance'] for p in performances]):.2f}\n"
        analysis += f"  æ ‡å‡†å·®: {np.std([p['performance'] for p in performances]):.2f}\n"
        
        return analysis
    
    def _create_si_prompt(
        self,
        instance_idx: int,
        instance_info: str,
        instance_analysis: str
    ) -> str:
        """
        åˆ›å»ºSIæç¤ºè¯
        ç›®æ ‡: ä¸ºç‰¹å®šå®ä¾‹ç”Ÿæˆä¸“ä¸šåŒ–å¯å‘å¼
        """
        prompt_template = PromptTemplate(
            input_variables=[
                "task_description",
                "instance_idx",
                "instance_info",
                "instance_analysis"
            ],
            template="""# Problem Context

{task_description}

# Task: Design a Specialized Heuristic for a Specific Instance

You are an expert in designing heuristic algorithms for combinatorial optimization problems.

## Situation

Current analysis shows that **Instance #{instance_idx}** has **low algorithmic differentiation** (low coefficient of variation), meaning:
- All existing heuristics perform similarly on this instance
- This instance is **under-served** by the current portfolio
- There is potential for a specialized heuristic to significantly improve performance

## Target Instance Information

{instance_info}

## Current Performance on This Instance

{instance_analysis}

## Your Task

Design a **NEW specialized heuristic** that is explicitly optimized for the structural characteristics of this specific instance type. Consider:

1. **Instance-Specific Patterns**: What makes this instance unique?
   - Size characteristics
   - Data distribution
   - Structural properties
   - Constraint patterns

2. **Why Existing Heuristics Underperform**: Analyze potential weaknesses
   - Are they too general?
   - Do they miss specific patterns?
   - Are there unexploited structures?

3. **Specialization Strategies**: Design targeted approaches
   - Exploit specific instance features
   - Use domain knowledge for this instance type
   - Implement custom decision rules
   - Optimize for this specific scale/structure

**IMPORTANT CODE FORMAT REQUIREMENTS:**

1. The function name must follow the pattern: `<strategy_name>_<random_4_chars>` (e.g., `specialized_greedy_k9m4`)
2. Follow the exact code format shown in the examples
3. Ensure your code is complete and executable
4. The heuristic should be **highly specialized** for this instance type

**Response Format:**

The response format is very important. Please respond in this format:

***python_code:
[Your complete Python code here]
***

**CRITICAL:** 
- Ensure there is no other content inside the ***
- Analysis and explanation outside *** are welcome
- The code must be complete and runnable
- Focus on instance-specific optimization

Please provide your specialized heuristic algorithm now:"""
        )
        
        # ç”Ÿæˆæœ€ç»ˆæç¤ºè¯
        final_prompt = prompt_template.format(
            task_description=self.task_description,
            instance_idx=instance_idx,
            instance_info=instance_info,
            instance_analysis=instance_analysis
        )
        
        return final_prompt
    
    def _call_llm(self, prompt: str) -> str:
        """è°ƒç”¨LLMç”Ÿæˆä»£ç """
        client = OpenAI(
            api_key=self.api_key,
            base_url="https://openrouter.ai/api/v1"  # OpenAI SDKä¼šè‡ªåŠ¨æ·»åŠ /chat/completions
        )
        
        try:
            response = client.chat.completions.create(
                model=self.model,
                messages=[
                    {
                        "role": "system",
                        "content": "You are an expert algorithm designer for combinatorial optimization problems."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                temperature=0.7
            )
            
            llm_response = response.choices[0].message.content
            return llm_response
            
        except Exception as e:
            print(f"âœ— APIè°ƒç”¨å¤±è´¥: {str(e)}")
            raise
    
    def _extract_code_from_response(self, response: str) -> str:
        """ä»LLMå“åº”ä¸­æå–ä»£ç """
        from src.run_hyper_heuristic.helper_function import extract_code_from_response
        return extract_code_from_response(response)
    
    def _save_generated_heuristic(self, code: str) -> str:
        """ä¿å­˜ç”Ÿæˆçš„å¯å‘å¼ä»£ç """
        from src.run_hyper_heuristic.helper_function import save_generated_heuristic
        return save_generated_heuristic(code, output_dir=self.output_dir)