"""
Complementary Portfolio Selection using Gurobi MILP
çº¯MILPç‰ˆæœ¬ - å®ç°è®ºæ–‡å…¬å¼(9)
"""

import gurobipy as gp
from gurobipy import GRB
import numpy as np
from typing import List, Dict


def complementary_selection_milp(
    pool: List[Dict],
    n: int,
    time_limit: float = 600.0,
    verbose: bool = True
) -> List[Dict]:
    """
    ä½¿ç”¨Gurobi MILPæ±‚è§£äº’è¡¥ç§ç¾¤é€‰æ‹©
    
    è®ºæ–‡å…¬å¼(9):
    min Î·
    s.t. Î£ x_h = n
         Î£ y_ih = 1, âˆ€i
         y_ih â‰¤ x_h, âˆ€i,h
         z_i = Î£ f_i(h)*y_ih, âˆ€i
         z_i â‰¤ Î·, âˆ€i
         x_h, y_ih âˆˆ {0,1}
    
    Args:
        pool: å€™é€‰å¯å‘å¼æ±  [{'name': ..., 'performance_vector': [...], 'avg_performance': ...}, ...]
        n: é€‰æ‹©æ•°é‡
        time_limit: æ—¶é—´é™åˆ¶(ç§’)
        verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
    
    Returns:
        selected: é€‰ä¸­çš„nä¸ªå¯å‘å¼
    """
    if len(pool) <= n:
        return pool
    
    H = len(pool)  # å€™é€‰æ•°é‡
    m = len(pool[0]['performance_vector'])  # å®ä¾‹æ•°é‡
    
    # æ€§èƒ½çŸ©é˜µ f[i][h]
    f = np.zeros((m, H))
    for h_idx, h in enumerate(pool):
        for i in range(m):
            f[i][h_idx] = h['performance_vector'][i]
    
    if verbose:
        print(f"\n{'='*80}")
        print(f"ğŸ”§ Gurobi MILP äº’è¡¥é€‰æ‹©")
        print(f"{'='*80}")
        print(f"å€™é€‰å¯å‘å¼: {H}, å®ä¾‹: {m}, é€‰æ‹©: {n}")
    
    # åˆ›å»ºæ¨¡å‹
    model = gp.Model("ComplementarySelection")
    model.setParam('OutputFlag', 1 if verbose else 0)
    model.setParam('TimeLimit', time_limit)
    model.setParam('MIPGap', 1e-4)
    
    # å˜é‡
    x = model.addVars(H, vtype=GRB.BINARY, name="x")
    y = model.addVars(m, H, vtype=GRB.BINARY, name="y")
    z = model.addVars(m, vtype=GRB.CONTINUOUS, lb=0, name="z")
    eta = model.addVar(vtype=GRB.CONTINUOUS, lb=0, name="eta")
    
    # ç›®æ ‡: min Î·
    model.setObjective(eta, GRB.MINIMIZE)
    
    # çº¦æŸ
    model.addConstr(gp.quicksum(x[h] for h in range(H)) == n)  # é€‰nä¸ª
    
    for i in range(m):
        model.addConstr(gp.quicksum(y[i, h] for h in range(H)) == 1)  # æ¯ä¸ªå®ä¾‹åˆ†é…ä¸€ä¸ª
        
        for h in range(H):
            model.addConstr(y[i, h] <= x[h])  # åªèƒ½åˆ†é…ç»™é€‰ä¸­çš„
        
        model.addConstr(z[i] == gp.quicksum(f[i][h] * y[i, h] for h in range(H)))  # æ€§èƒ½
        model.addConstr(z[i] <= eta)  # worst-case
    
    # æ±‚è§£
    if verbose:
        print("å¼€å§‹æ±‚è§£...")
    
    model.optimize()
    
    # æå–è§£
    if model.status in [GRB.OPTIMAL, GRB.TIME_LIMIT]:
        if verbose:
            if model.status == GRB.OPTIMAL:
                print(f"âœ… æœ€ä¼˜è§£! Î·* = {eta.X:.4f}, æ—¶é—´: {model.Runtime:.2f}ç§’")
            else:
                print(f"âš ï¸  æ—¶é—´é™åˆ¶, Gap: {model.MIPGap*100:.2f}%")
        
        selected_indices = [h for h in range(H) if x[h].X > 0.5]
        selected = [pool[h] for h in selected_indices]
        
        if verbose:
            print(f"\né€‰ä¸­çš„å¯å‘å¼:")
            for i, h_idx in enumerate(selected_indices, 1):
                print(f"  {i}. {pool[h_idx]['name']}")
            print(f"{'='*80}\n")
        
        return selected
    else:
        raise RuntimeError(f"MILPæ±‚è§£å¤±è´¥: status = {model.status}")